{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4be7e2f6-a815-4d83-bc57-42a78b962c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amelia\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">96,768</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">7,740</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m96,768\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)             │         \u001b[38;5;34m7,740\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">236,092</span> (922.23 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m236,092\u001b[0m (922.23 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">236,092</span> (922.23 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m236,092\u001b[0m (922.23 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "334/334 - 217s - 650ms/step - loss: 1.6331 - val_loss: 1.1406\n",
      "Epoch 2/100\n",
      "334/334 - 277s - 829ms/step - loss: 1.0750 - val_loss: 0.9605\n",
      "Epoch 3/100\n",
      "334/334 - 388s - 1s/step - loss: 1.0429 - val_loss: 0.8382\n",
      "Epoch 4/100\n",
      "334/334 - 410s - 1s/step - loss: 1.0322 - val_loss: 0.8279\n",
      "Epoch 5/100\n",
      "334/334 - 395s - 1s/step - loss: 1.0286 - val_loss: 0.8133\n",
      "Epoch 6/100\n",
      "334/334 - 398s - 1s/step - loss: 1.0268 - val_loss: 0.8131\n",
      "Epoch 7/100\n",
      "334/334 - 399s - 1s/step - loss: 1.0249 - val_loss: 0.8009\n",
      "Epoch 8/100\n",
      "334/334 - 395s - 1s/step - loss: 1.0231 - val_loss: 0.7920\n",
      "Epoch 9/100\n",
      "334/334 - 384s - 1s/step - loss: 1.0206 - val_loss: 0.7822\n",
      "Epoch 10/100\n",
      "334/334 - 361s - 1s/step - loss: 1.0190 - val_loss: 0.7757\n",
      "Epoch 11/100\n",
      "334/334 - 361s - 1s/step - loss: 1.0165 - val_loss: 0.7677\n",
      "Epoch 12/100\n",
      "334/334 - 362s - 1s/step - loss: 1.0049 - val_loss: 0.7595\n",
      "Epoch 13/100\n",
      "334/334 - 361s - 1s/step - loss: 0.9840 - val_loss: 0.7506\n",
      "Epoch 14/100\n",
      "334/334 - 366s - 1s/step - loss: 0.9731 - val_loss: 0.7356\n",
      "Epoch 15/100\n",
      "334/334 - 378s - 1s/step - loss: 0.9496 - val_loss: 0.6995\n",
      "Epoch 16/100\n",
      "334/334 - 380s - 1s/step - loss: 0.9344 - val_loss: 0.7145\n",
      "Epoch 17/100\n",
      "334/334 - 385s - 1s/step - loss: 0.9208 - val_loss: 0.6705\n",
      "Epoch 18/100\n",
      "334/334 - 361s - 1s/step - loss: 0.9069 - val_loss: 0.6626\n",
      "Epoch 19/100\n",
      "334/334 - 363s - 1s/step - loss: 0.8979 - val_loss: 0.6514\n",
      "Epoch 20/100\n",
      "334/334 - 361s - 1s/step - loss: 0.8931 - val_loss: 0.6476\n",
      "Epoch 21/100\n",
      "334/334 - 359s - 1s/step - loss: 0.8964 - val_loss: 0.6538\n",
      "Epoch 22/100\n",
      "334/334 - 359s - 1s/step - loss: 0.9834 - val_loss: 0.7095\n",
      "Epoch 23/100\n",
      "334/334 - 362s - 1s/step - loss: 0.9101 - val_loss: 0.6690\n",
      "Epoch 24/100\n",
      "334/334 - 359s - 1s/step - loss: 0.9063 - val_loss: 0.6593\n",
      "Epoch 25/100\n",
      "334/334 - 361s - 1s/step - loss: 0.8929 - val_loss: 0.6444\n",
      "Epoch 26/100\n",
      "334/334 - 359s - 1s/step - loss: 0.8902 - val_loss: 0.6361\n",
      "Epoch 27/100\n",
      "334/334 - 363s - 1s/step - loss: 0.8896 - val_loss: 0.6289\n",
      "Epoch 28/100\n",
      "334/334 - 377s - 1s/step - loss: 0.8858 - val_loss: 0.6324\n",
      "Epoch 29/100\n",
      "334/334 - 358s - 1s/step - loss: 0.8829 - val_loss: 0.6172\n",
      "Epoch 30/100\n",
      "334/334 - 359s - 1s/step - loss: 0.8732 - val_loss: 0.6403\n",
      "Epoch 31/100\n",
      "334/334 - 360s - 1s/step - loss: 0.8632 - val_loss: 0.5935\n",
      "Epoch 32/100\n",
      "334/334 - 361s - 1s/step - loss: 0.8585 - val_loss: 0.6002\n",
      "Epoch 33/100\n",
      "334/334 - 358s - 1s/step - loss: 0.8545 - val_loss: 0.5821\n",
      "Epoch 34/100\n",
      "334/334 - 358s - 1s/step - loss: 0.8486 - val_loss: 0.5764\n",
      "Epoch 35/100\n",
      "334/334 - 362s - 1s/step - loss: 0.8454 - val_loss: 0.5756\n",
      "Epoch 36/100\n",
      "334/334 - 360s - 1s/step - loss: 0.8441 - val_loss: 0.5717\n",
      "Epoch 37/100\n",
      "334/334 - 365s - 1s/step - loss: 0.8424 - val_loss: 0.5713\n",
      "Epoch 38/100\n",
      "334/334 - 358s - 1s/step - loss: 0.8382 - val_loss: 0.5636\n",
      "Epoch 39/100\n",
      "334/334 - 359s - 1s/step - loss: 0.8377 - val_loss: 0.5618\n",
      "Epoch 40/100\n",
      "334/334 - 362s - 1s/step - loss: 0.8336 - val_loss: 0.5598\n",
      "Epoch 41/100\n",
      "334/334 - 361s - 1s/step - loss: 0.8347 - val_loss: 0.5521\n",
      "Epoch 42/100\n",
      "334/334 - 367s - 1s/step - loss: 0.8310 - val_loss: 0.5564\n",
      "Epoch 43/100\n",
      "334/334 - 359s - 1s/step - loss: 0.8301 - val_loss: 0.5517\n",
      "Epoch 44/100\n",
      "334/334 - 360s - 1s/step - loss: 0.8280 - val_loss: 0.5480\n",
      "Epoch 45/100\n",
      "334/334 - 382s - 1s/step - loss: 0.8281 - val_loss: 0.5466\n",
      "Epoch 46/100\n",
      "334/334 - 359s - 1s/step - loss: 0.8275 - val_loss: 0.5465\n",
      "Epoch 47/100\n",
      "334/334 - 384s - 1s/step - loss: 0.8256 - val_loss: 0.5483\n",
      "Epoch 48/100\n",
      "334/334 - 360s - 1s/step - loss: 0.8244 - val_loss: 0.5475\n",
      "Epoch 49/100\n",
      "334/334 - 359s - 1s/step - loss: 0.8235 - val_loss: 0.5448\n",
      "Epoch 50/100\n",
      "334/334 - 360s - 1s/step - loss: 0.8222 - val_loss: 0.5456\n",
      "Epoch 51/100\n",
      "334/334 - 361s - 1s/step - loss: 0.8192 - val_loss: 0.5427\n",
      "Epoch 52/100\n",
      "334/334 - 359s - 1s/step - loss: 0.8193 - val_loss: 0.5396\n",
      "Epoch 53/100\n",
      "334/334 - 381s - 1s/step - loss: 0.8185 - val_loss: 0.5430\n",
      "Epoch 54/100\n",
      "334/334 - 422s - 1s/step - loss: 0.8171 - val_loss: 0.5457\n",
      "Epoch 55/100\n",
      "334/334 - 409s - 1s/step - loss: 0.8166 - val_loss: 0.5401\n",
      "Epoch 56/100\n",
      "334/334 - 398s - 1s/step - loss: 0.8154 - val_loss: 0.5393\n",
      "Epoch 57/100\n",
      "334/334 - 406s - 1s/step - loss: 0.8136 - val_loss: 0.5379\n",
      "Epoch 58/100\n",
      "334/334 - 411s - 1s/step - loss: 0.8151 - val_loss: 0.5368\n",
      "Epoch 59/100\n",
      "334/334 - 442s - 1s/step - loss: 0.8134 - val_loss: 0.5425\n",
      "Epoch 60/100\n",
      "334/334 - 405s - 1s/step - loss: 0.8109 - val_loss: 0.5388\n",
      "Epoch 61/100\n",
      "334/334 - 406s - 1s/step - loss: 0.8108 - val_loss: 0.5382\n",
      "Epoch 62/100\n",
      "334/334 - 397s - 1s/step - loss: 0.8093 - val_loss: 0.5379\n",
      "Epoch 63/100\n",
      "334/334 - 398s - 1s/step - loss: 0.8083 - val_loss: 0.5345\n",
      "Epoch 64/100\n",
      "334/334 - 395s - 1s/step - loss: 0.8086 - val_loss: 0.5390\n",
      "Epoch 65/100\n",
      "334/334 - 397s - 1s/step - loss: 0.8072 - val_loss: 0.5352\n",
      "Epoch 66/100\n",
      "334/334 - 401s - 1s/step - loss: 0.8070 - val_loss: 0.5340\n",
      "Epoch 67/100\n",
      "334/334 - 446s - 1s/step - loss: 0.8056 - val_loss: 0.5363\n",
      "Epoch 68/100\n",
      "334/334 - 394s - 1s/step - loss: 0.8050 - val_loss: 0.5345\n",
      "Epoch 69/100\n",
      "334/334 - 381s - 1s/step - loss: 0.8045 - val_loss: 0.5339\n",
      "Epoch 70/100\n",
      "334/334 - 396s - 1s/step - loss: 0.8036 - val_loss: 0.5356\n",
      "Epoch 71/100\n",
      "334/334 - 396s - 1s/step - loss: 0.8021 - val_loss: 0.5345\n",
      "Epoch 72/100\n",
      "334/334 - 393s - 1s/step - loss: 0.8002 - val_loss: 0.5393\n",
      "Epoch 73/100\n",
      "334/334 - 450s - 1s/step - loss: 0.8007 - val_loss: 0.5323\n",
      "Epoch 74/100\n",
      "334/334 - 404s - 1s/step - loss: 0.8013 - val_loss: 0.5304\n",
      "Epoch 75/100\n",
      "334/334 - 398s - 1s/step - loss: 0.7977 - val_loss: 0.5352\n",
      "Epoch 76/100\n",
      "334/334 - 400s - 1s/step - loss: 0.7991 - val_loss: 0.5348\n",
      "Epoch 77/100\n",
      "334/334 - 369s - 1s/step - loss: 0.7976 - val_loss: 0.5341\n",
      "Epoch 78/100\n",
      "334/334 - 368s - 1s/step - loss: 0.7971 - val_loss: 0.5339\n",
      "Epoch 79/100\n",
      "334/334 - 365s - 1s/step - loss: 0.7967 - val_loss: 0.5319\n",
      "Epoch 80/100\n",
      "334/334 - 369s - 1s/step - loss: 0.7959 - val_loss: 0.5330\n",
      "Epoch 81/100\n",
      "334/334 - 365s - 1s/step - loss: 0.7963 - val_loss: 0.5306\n",
      "Epoch 82/100\n",
      "334/334 - 365s - 1s/step - loss: 0.7942 - val_loss: 0.5316\n",
      "Epoch 83/100\n",
      "334/334 - 370s - 1s/step - loss: 0.7925 - val_loss: 0.5349\n",
      "Epoch 84/100\n",
      "334/334 - 370s - 1s/step - loss: 0.7924 - val_loss: 0.5346\n",
      "Epoch 85/100\n",
      "334/334 - 364s - 1s/step - loss: 0.7921 - val_loss: 0.5348\n",
      "Epoch 86/100\n",
      "334/334 - 366s - 1s/step - loss: 0.7914 - val_loss: 0.5324\n",
      "Epoch 87/100\n",
      "334/334 - 370s - 1s/step - loss: 0.7910 - val_loss: 0.5323\n",
      "Epoch 88/100\n",
      "334/334 - 368s - 1s/step - loss: 0.7908 - val_loss: 0.5325\n",
      "Epoch 89/100\n",
      "334/334 - 367s - 1s/step - loss: 0.7887 - val_loss: 0.5310\n",
      "Epoch 90/100\n",
      "334/334 - 368s - 1s/step - loss: 0.7881 - val_loss: 0.5339\n",
      "Epoch 91/100\n",
      "334/334 - 364s - 1s/step - loss: 0.7881 - val_loss: 0.5340\n",
      "Epoch 92/100\n",
      "334/334 - 384s - 1s/step - loss: 0.7885 - val_loss: 0.5326\n",
      "Epoch 93/100\n",
      "334/334 - 414s - 1s/step - loss: 0.7880 - val_loss: 0.5312\n",
      "Epoch 94/100\n",
      "334/334 - 393s - 1s/step - loss: 0.7860 - val_loss: 0.5300\n",
      "Epoch 95/100\n",
      "334/334 - 453s - 1s/step - loss: 0.7860 - val_loss: 0.5355\n",
      "Epoch 96/100\n",
      "334/334 - 400s - 1s/step - loss: 0.7844 - val_loss: 0.5323\n",
      "Epoch 97/100\n",
      "334/334 - 401s - 1s/step - loss: 0.7827 - val_loss: 0.5329\n",
      "Epoch 98/100\n",
      "334/334 - 413s - 1s/step - loss: 0.7833 - val_loss: 0.5303\n",
      "Epoch 99/100\n",
      "334/334 - 414s - 1s/step - loss: 0.7825 - val_loss: 0.5333\n",
      "Epoch 100/100\n",
      "334/334 - 370s - 1s/step - loss: 0.7810 - val_loss: 0.5315\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "#INPUT_FILE_NAME = 'C:/Users/Amelia/Documents/Computer Science Spring 2025/Neural Networks/GoShortenedExampleData.txt'\n",
    "INPUT_FILE_NAME = 'C:/Users/Amelia/Documents/Computer Science Spring 2025/Neural Networks/Jan1940-Dec1944.txt'\n",
    "WINDOW_LENGTH = 40\n",
    "WINDOW_STEP = 3\n",
    "BEAM_SIZE = 8\n",
    "NEXT_COORDINATES = 4\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "file = open(INPUT_FILE_NAME, 'r', encoding='utf-8')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# Make lowercase and remove newline and extra spaces.\n",
    "#text = text.lower() \n",
    "text = text.replace('\\n', ' ')\n",
    "text = text.replace('  ', ' ')\n",
    "text = text.replace('B', ' ')\n",
    "text = text.replace('W', ' ')\n",
    "text = text.replace(';', ' ')\n",
    "\n",
    "# Encode characters as indices.\n",
    "unique_chars = list(set(text))\n",
    "char_to_index = dict((ch, index) for index, ch \n",
    "                     in enumerate(unique_chars))\n",
    "index_to_char = dict((index, ch) for index, ch \n",
    "                     in enumerate(unique_chars))\n",
    "encoding_width = len(char_to_index)\n",
    "\n",
    "fragments = []\n",
    "targets = []\n",
    "for i in range(0, len(text) - WINDOW_LENGTH, WINDOW_STEP):\n",
    "    fragments.append(text[i: i + WINDOW_LENGTH])\n",
    "    targets.append(text[i + WINDOW_LENGTH])\n",
    "# Convert to one-hot encoded training data.\n",
    "X = np.zeros((len(fragments), WINDOW_LENGTH, encoding_width))\n",
    "y = np.zeros((len(fragments), encoding_width))\n",
    "for i, fragment in enumerate(fragments):\n",
    "    for j, char in enumerate(fragment):\n",
    "        X[i, j, char_to_index[char]] = 1\n",
    "    target_char = targets[i]\n",
    "    y[i, char_to_index[target_char]] = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True,\n",
    "               dropout=0.2, recurrent_dropout=0.2,\n",
    "               input_shape=(None, encoding_width)))\n",
    "model.add(LSTM(128, dropout=0.2,\n",
    "               recurrent_dropout=0.2))\n",
    "model.add(Dense(encoding_width, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam')\n",
    "model.summary()\n",
    "history = model.fit(X, y, validation_split=0.05,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS, verbose=2,\n",
    "                    shuffle=True)\n",
    "\n",
    "#text prediction\n",
    "\n",
    "next_move = ' ' \n",
    "one_hots = []\n",
    "for i, char in enumerate(next_move):\n",
    "    x = np.zeros(encoding_width)\n",
    "    x[char_to_index[char]] = 1\n",
    "    one_hots.append(x)\n",
    "beams = [(np.log(1.0), next_move, one_hots)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3b7cd1d-bfc6-4310-933a-2a02034c6498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [fb]  [fb]  [fb]  [fb]  [fb]  [fb]  \n",
      " [db]  [db]  [cb]  [bb]  [bb]  [bb]  \n",
      " [db]  [db]  [cb]  [cb]  [bb]  [bb]  \n",
      " [cb]  [db]  [cb]  [bb]  [bb]  [bb]  \n",
      " [fb]  [fb]  [fb]  [fb]  [fb]  [gb]  \n",
      " [db]  [db]  [db]  [cb]  [bb]  [bb]  \n",
      " [cb]  [db]  [cb]  [cb]  [bb]  [bb]  \n",
      " [fb]  [fb]  [gb]  [fb]  [fb]  [fb]  \n",
      " [fb]  [gb]  [fb]  [fb]  [fb]  [fb]  \n",
      " [cb]  [db]  [db]  [cb]  [bb]  [bb]  \n",
      " [fb]  [fb]  [fb]  [gb]  [fb]  [fb]  \n",
      " [fb]  [fb]  [fb]  [fb]  [gb]  [fb]  \n",
      " [fb]  [fc]  [gb]  [fb]  [fb]  [fb]  \n",
      " [db]  [db]  [cb]  [cb]  [cb]  [bb]  \n",
      " [fb]  [fb]  [fb]  [fb]  [fb]  [eb]  \n",
      " [db]  [db]  [db]  [cb]  [cb]  [bb]  \n",
      " [db]  [db]  [db]  [bb]  [bb]  [bb]  \n",
      " [db]  [db]  [cb]  [bc]  [bb]  [bb]  \n",
      " [cb]  [cb]  [bb]  [bb]  [bb]  [bb]  \n",
      " [cb]  [cb]  [cb]  [bb]  [bb]  [bb]  \n",
      " [db]  [db]  [cb]  [bb]  [bc]  [bb]  \n",
      " [db]  [db]  [cb]  [db]  [cb]  [bb]  \n",
      " [db]  [db]  [cb]  [bb]  [bb]  [cb]  \n",
      " [cb]  [db]  [db]  [bb]  [bb]  [bb]  \n",
      " [db]  [db]  [db]  [db]  [cb]  [bb]  \n",
      " [db]  [db]  [cb]  [db]  [bb]  [bb]  \n",
      " [cb]  [db]  [cb]  [cb]  [cb]  [bb]  \n",
      " [db]  [db]  [cb]  [bb]  [cb]  [bb]  \n",
      " [db]  [cb]  [cb]  [bb]  [bb]  [bb]  \n",
      " [cb]  [db]  [db]  [cb]  [cb]  [bb]  \n",
      " [cb]  [db]  [bb]  [bb]  [bb]  [bb]  \n",
      " [fb]  [fb]  [fb]  [fb]  [gb]  [hb]  \n",
      " [cb]  [db]  [cb]  [bb]  [bc]  [bb]  \n",
      " [cb]  [db]  [cb]  [db]  [cb]  [bb]  \n",
      " [fb]  [fb]  [fb]  [fb]  [gb]  [gb]  \n",
      " [db]  [db]  [cb]  [cb]  [bb]  [bc]  \n",
      " [db]  [db]  [cb]  [cb]  [bb]  [cb]  \n",
      " [db]  [db]  [cb]  [bb]  [bb]  [bc]  \n",
      " [cb]  [db]  [cb]  [bb]  [bb]  [cb]  \n",
      " [db]  [cb]  [bb]  [bb]  [bb]  [bb]  \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "model.save('saved_model.h5')\n",
    "# path_to_dir = 'C:/Users/Amelia/Documents/Computer Science Spring 2025/Neural Networks/model'\n",
    "load_model = tf.keras.models.load_model('saved_model.h5')\n",
    "\n",
    "BEAM_SIZE = 40\n",
    "\n",
    "# Predict next coordinates for game.\n",
    "for i in range(NEXT_COORDINATES):\n",
    "    minibatch_list = []\n",
    "    # Create minibatch from one-hot encodings, and predict.\n",
    "    for triple in beams:\n",
    "        minibatch_list.append(triple[2])\n",
    "    minibatch = np.array(minibatch_list)\n",
    "    y_predict = load_model.predict(minibatch, verbose=0)\n",
    "    new_beams = []\n",
    "    for j, softmax_vec in enumerate(y_predict):\n",
    "        triple = beams[j]\n",
    "        # Create BEAM_SIZE new beams from each existing beam.\n",
    "        for k in range(BEAM_SIZE):\n",
    "            char_index = np.argmax(softmax_vec)\n",
    "            new_prob = triple[0] + np.log(\n",
    "                softmax_vec[char_index])\n",
    "            new_letters = triple[1] + index_to_char[char_index]\n",
    "            x = np.zeros(encoding_width)\n",
    "            x[char_index] = 1\n",
    "            new_one_hots = triple[2].copy()\n",
    "            new_one_hots.append(x)\n",
    "            new_beams.append((new_prob, new_letters,\n",
    "                              new_one_hots))\n",
    "            softmax_vec[char_index] = 0\n",
    "    # Prune tree to only keep BEAM_SIZE most probable beams.\n",
    "    new_beams.sort(key=lambda tup: tup[0], reverse=True)\n",
    "    beams = new_beams[0:BEAM_SIZE]\n",
    "for item in beams:\n",
    "    print(item[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ec4adbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [db]  [cb]  [bb]  [bb]  [bb]  [bb]  '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = str(item[1])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ebe597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
